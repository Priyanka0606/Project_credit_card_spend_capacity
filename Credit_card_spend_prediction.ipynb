{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95014383",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d696b3",
   "metadata": {},
   "source": [
    "Develop a predictive framework to gauge the overall credit card spend capacity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa8075",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81957eee",
   "metadata": {},
   "source": [
    "## Setting display options to ensure feature name visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a207ecb",
   "metadata": {},
   "source": [
    "## Importing the Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b44163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('/Users/priyankac/Downloads/Projects/Data.xlsx - customer_dbase.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the size of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the different data types in the dataset\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3325a7",
   "metadata": {},
   "source": [
    "## Drop any ID kind of Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['custid'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74081f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5b566",
   "metadata": {},
   "source": [
    "## Defining Independent and Target Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de52e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the features 'cardspent' and 'card2spent' to give a feature 'totspend'\n",
    "df['totspend'] = df['cardspent'] + df['card2spent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e80ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'cardspent' and 'card2spent' from our dataframe\n",
    "df = df.drop(['cardspent', 'card2spent'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd6e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of the column 'totspend'\n",
    "df['totspend'].plot(kind = 'hist')\n",
    "\n",
    "# The data is skewed\n",
    "# There are outliers\n",
    "# there is no normal distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b608ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing log transformation of the above feature\n",
    "df['logtotspend'] = np.log(df['totspend'])\n",
    "df['logtotspend'].plot(kind = 'hist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the parent feature 'totspend' as log transformation looks more normally distributed\n",
    "df = df.drop(['totspend'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3866740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes containing the Independent and Dependent features\n",
    "X = df.drop(['logtotspend'], axis = 1)\n",
    "Y = df[['logtotspend']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9696b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86ab75",
   "metadata": {},
   "source": [
    "## Split the features into Numerical and Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the provided data dictionary we know the categorical variables\n",
    "cat_var = ['region','townsize','gender','agecat','birthmonth','edcat','jobcat','union',\n",
    "'employ','empcat','retire','inccat','default','jobsat','marital','spousedcat',\n",
    "'homeown','hometype','address','addresscat','cars','carown','cartype',\n",
    "'carcatvalue','carbought','carbuy','commute','commutecat','commutecar','commutemotorcycle',\n",
    "'commutecarpool','commutebus','commuterail','commutepublic','commutebike','commutewalk',\n",
    "'commutenonmotor','telecommute','reason','polview','polparty','polcontrib','vote','card',\n",
    "'cardtype','cardbenefit','cardfee','cardtenure','cardtenurecat','card2','card2type',\n",
    "'card2benefit','card2fee','card2tenure','card2tenurecat','active','bfast','churn','tollfree',\n",
    "'equip','callcard','wireless','multline','voice','pager','internet','callid','callwait','forward',\n",
    "'confer','ebill','owntv','ownvcr','owndvd','owncd','ownpda','ownpc','ownipod','owngame','ownfax','news',\n",
    "'response_01','response_02','response_03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d621a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of numerical features\n",
    "numerical_var = []\n",
    "for i in X.columns:\n",
    "    if i not in cat_var:\n",
    "        numerical_var.append(i)\n",
    "print(numerical_var)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the numerical and categorical dataframes\n",
    "num = X.drop(cat_var, axis = 1)\n",
    "char = X.drop(numerical_var, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the categorical columns into type 'object' as they look like numbers\n",
    "all_columns = list(char) # creates list of all column headers\n",
    "char[all_columns] = char[all_columns].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df43176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of numerical features : ', num.shape)\n",
    "print('Shape of categorical features : ', char.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ca73a",
   "metadata": {},
   "source": [
    "## Check for Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.describe(percentiles = [0.01,0.05,0.10,0.25,0.50,0.75,0.95,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868de5a",
   "metadata": {},
   "source": [
    "## Removal of Extreme Values and Outliers from the Numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_cap(x):\n",
    "    x = x.clip(lower = x.quantile(0.01))\n",
    "    x = x.clip(upper = x.quantile(0.99))\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = num.apply(lambda x: outlier_cap(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40faf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.describe(percentiles = [0.01,0.05,0.10,0.25,0.50,0.75,0.95,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123d8a8",
   "metadata": {},
   "source": [
    "## Missing Values handling - Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93911205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only those columns that have >= 75% data populated\n",
    "num = num.loc[:, num.isnull().mean() <= 0.25]\n",
    "num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab40f3",
   "metadata": {},
   "source": [
    "## Missing Value Imputation - Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77145d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "num_1 = pd.DataFrame(imputer.fit_transform(num), index = num.index, columns = num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b87fce",
   "metadata": {},
   "source": [
    "## Missing Values Handling - Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42370365",
   "metadata": {},
   "outputs": [],
   "source": [
    "char.isnull().mean().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9b1c8",
   "metadata": {},
   "source": [
    "## Encode Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a31ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_encode = pd.get_dummies(char, drop_first = True)\n",
    "char_encode.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5eb54",
   "metadata": {},
   "source": [
    "## Build the complete feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pd.concat([char_encode, num_1], axis = 1, join = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has a large number of features\n",
    "# Using the Random Forest Regresssor to find important features \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf_rf = RandomForestRegressor(n_estimators =20)\n",
    "clf_rf.fit(X_all, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(clf_rf.feature_importances_,\n",
    "                                  index = X_all.columns,\n",
    "                                  columns = ['importance']).sort_values('importance', ascending = False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the top 40 important faetures\n",
    "feature_list = feature_importances.iloc[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5387cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = list(feature_list.index)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retaining the top 40 important features in the X_all dataframe\n",
    "X_all = X_all[top_features]\n",
    "X_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8b7bc",
   "metadata": {},
   "source": [
    "## Let us perform Variable Clustering on the data to eliminate correlation among features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VarClusHi\n",
    "!pip install varclushi==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from varclushi import VarClusHi\n",
    "vc = VarClusHi(X_all, maxeigval2 = 1, maxclus = 8)\n",
    "vc.varclus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2238d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = vc.rsquare\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features from each cluster having the lowest RS_ratio\n",
    "# A variable selected from each cluster should have a high correlation with\n",
    "# its own cluster and a low correlation with the other clusters\n",
    "temp = check.groupby('Cluster')['RS_Ratio'].agg(['min'])\n",
    "temp.columns = ['RS_Ratio']\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the 'RS_Ratio' with the 'check' dataframe to get the feature names\n",
    "filter = temp.merge(check, how = 'left', on = 'RS_Ratio')\n",
    "filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111daaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = filter['Variable']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06cdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the final_features to create the final dataframe\n",
    "X_final = X_all[final_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of final dataset\n",
    "X_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aefc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2257aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below snippet is to check for target variable discrimination as compared to the varying entries\n",
    "# of the X-variable\n",
    "check = pd.concat([X_final,Y], axis = 1, join = 'inner')\n",
    "check.groupby('pets')['logtotspend'].agg(['min', 'mean', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea61374",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.concat([X_final,Y], axis = 1, join = 'inner')\n",
    "check.groupby('card_3.0')['logtotspend'].agg(['min', 'mean', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013f4c0",
   "metadata": {},
   "source": [
    "## Splitting the data into Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, Y, test_size = 0.2, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a59f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Training Data : ', X_train.shape)\n",
    "print('Shape of Testing Data : ', X_test.shape)\n",
    "print('Average Salary in Training Data : ', y_train.mean())\n",
    "print('Aversge Salary in Testing Data : ', y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc632ab",
   "metadata": {},
   "source": [
    "## Model Building Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab159560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Decision Tree Model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtree = DecisionTreeRegressor(random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min_Samples_Split starting from 5% of training base\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_dist = {'max_depth': [3,4, 5, 6, 7,8], 'min_samples_split': [175,200,225,250,275,300] }\n",
    "tree_grid = GridSearchCV(dtree, cv = 10, param_grid=param_dist,n_jobs = -1)\n",
    "tree_grid.fit(X_train,y_train) \n",
    "print('Best Parameters using grid search: \\n', tree_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree=DecisionTreeRegressor(random_state = 20, max_depth = 4, min_samples_split = 225)\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Random Forest Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf=RandomForestRegressor(n_estimators=20)\n",
    "rf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b6e47",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426d416",
   "metadata": {},
   "source": [
    "### R Square Metric Between Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_pred_train = dtree.predict(X_train)\n",
    "dtree_pred_test = dtree.predict(X_test)\n",
    "dtree_pred_final = dtree.predict(X_final)\n",
    "\n",
    "X_final['pred_totspend_tree'] = pd.DataFrame(dtree_pred_final, index = X_final.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_train = rf.predict(X_train)\n",
    "rf_pred_test = rf.predict(X_test)\n",
    "rf_pred_final = rf.predict(X_final)\n",
    "\n",
    "X_final['pred_totspend_rf'] = pd.DataFrame(rf_pred_final, index = X_final.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98088431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R Sqaure for Decision Tree\n",
    "from sklearn.metrics import r2_score\n",
    "r_sq_train=r2_score(dtree_pred_train,y_train)\n",
    "r_sq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r_sq_test=r2_score(dtree_pred_test,y_test)\n",
    "r_sq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R Square for Random Forest\n",
    "from sklearn.metrics import r2_score\n",
    "r_sq_train=r2_score(rf_pred_train,y_train)\n",
    "r_sq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8069a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r_sq_test=r2_score(rf_pred_test,y_test)\n",
    "r_sq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest performs better than Decision Tree, still R sqaure is not good in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('MSE for Test:',metrics.mean_squared_error(rf_pred_test,y_test))\n",
    "print('MSE for Train:',metrics.mean_squared_error(rf_pred_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4c343",
   "metadata": {},
   "source": [
    "## Visualizing the Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe160cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = pd.concat([X_final, Y], axis = 1, join = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bb46e",
   "metadata": {},
   "source": [
    "## Create Buckets of Data Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c39a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval['totspend_rank']=pd.qcut(data_eval['logtotspend'].rank(method='first').values,50,duplicates='drop').codes+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd862a4",
   "metadata": {},
   "source": [
    "## Plot the Actuals versus Predicted across those buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='totspend_rank', y='logtotspend', data=data_eval, color='Blue')\n",
    "ax = sns.lineplot(x='totspend_rank', y='pred_totspend_rf', data=data_eval, color='Red')\n",
    "\n",
    "# Although the model did not perform very well on the test data,\n",
    "# but from the graph we can see that it is very close to predicting the high spending customers\n",
    "# Cannot rely exactly on the predicted value but can rely on the directional segmentation of the predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a357d41",
   "metadata": {},
   "source": [
    "## Error Cluster Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Error Cluster Capture Analysis to find the extent to which the model is performing the best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ba3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get error percentage between Predicted and Actual Values\n",
    "data_eval['error'] = data_eval['pred_totspend_rf']-data_eval['logtotspend']\n",
    "data_eval['error_percentage'] = (data_eval['error']/df['logtotspend']).abs()\n",
    "error_df = data_eval[['error_percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build error cluster based on similar error values\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "bins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')\n",
    "error_df_bin = pd.DataFrame(bins.fit_transform(error_df), index = error_df.index, \n",
    "                            columns = error_df.columns).add_suffix('_bin')\n",
    "error_df = pd.concat([error_df, error_df_bin], axis = 1, join = 'inner')\n",
    "error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the kmeans cluster to find for how many rows in the data do we have the lowest error percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the error clusters for observation capture\n",
    "model_eval= error_df.groupby('error_percentage_bin')['error_percentage'].agg(['min','max','mean','count'])\n",
    "model_eval['cum_count'] = model_eval['count'].cumsum()\n",
    "model_eval['cum_count_prop'] = model_eval['cum_count']/max(model_eval['cum_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae23c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cumulative Observation Capture by Error Clusters\n",
    "ax=sns.lineplot(x=model_eval.index,y='cum_count_prop',data=model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.barplot(x=model_eval.index,y='mean',data=model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb785c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
